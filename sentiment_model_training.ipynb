{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial News Headline Sentiment Classifier\n",
    "\n",
    "Cody Lange\n",
    "August 8th, 2023\n",
    "\n",
    "**Motivation**\n",
    "Market sentiment derived from news articles is one factor that can influence trading and investment decisions. By understanding the sentiment of financial news headlines, investors, traders, and financial institutions have another tool that they can use to make more informed decisions, assess risks, and potentially even design algorithmic trading strategies. This notebook demonstrates the process of training a BERT (Bidirectional Encoder Representations from Transformers) model on the financial_phrasebank dataset to classify Yahoo Finance headlines into three sentiment categories: positive, neutral, and negative. While this exercise uses financial headlines as an input, the underlying techniques used in this notebook can be transferred to simmilar text classification tasks.\n",
    "The methods outlined in this notebook also serve as a less-complicated, indirect sample for my actual data science work making a similar transformer model for multilabel text classification.\n",
    "\n",
    "**Data Source**\n",
    "The financial_phrasebank dataset is a collection of financial news headlines for companies listed in OMX Helsinki, each labeled with a sentiment score of 2 for positive, 1 for neutral, or 0 for negative. Labels were decided upon by a group of 13 annotators at the  Aalto University School of Business in which 75% of the judges agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc as pb\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Show all dataframe columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset financial_phrasebank (C:/Users/langecod/.cache/huggingface/datasets/financial_phrasebank/sentences_75agree/1.0.0/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45f9600b63a428ab6ad8114d0366339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Load the datasets into a pandas dataframe\n",
    "headlines = pd.DataFrame( load_dataset(\"financial_phrasebank\", \"sentences_75agree\")[\"train\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In the third quarter of 2010 , net sales incre...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Operating profit rose to EUR 13.1 mn from EUR ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  According to Gran , the company has no plans t...      1\n",
       "1  With the new production plant the company woul...      2\n",
       "2  For the last quarter of 2010 , Componenta 's n...      2\n",
       "3  In the third quarter of 2010 , net sales incre...      2\n",
       "4  Operating profit rose to EUR 13.1 mn from EUR ...      2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3453 entries, 0 to 3452\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  3453 non-null   object\n",
      " 1   label     3453 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 54.1+ KB\n"
     ]
    }
   ],
   "source": [
    "headlines.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the occurrences of each label\n",
    "label_counts = headlines['label'].value_counts()\n",
    "\n",
    "# Create a bar plot of the label distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(label_counts.index, label_counts.values)\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Label Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less is more when training a transformer model, such as RoBERTa, for text classification. The basic concept behind transformers is to learn optimal representations of language by understanding the contexts, nuances, and intricacies present in the text. This is achieved by representing text as a sequence of tokens and using an attention mechanism to learn the effect of each token on the others in the sequence. Training the model on unprocessed text preserves the semantic richness of the language, which would otherwise be parsed out through the removal of special characters and stop words, stemming, lemmatization, etc. While text cleaning is warranted for simpler models that benefit from uniformity, dimensionality reduction, and noise removal, (see Tf-IDF, count vectorization, etc.) opting out of text processing is advantageous here since the news headlines are short enough to meet the 512-token input limit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check: Do our Inputs Meet the 512 Token Limit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([1222, 1654,  729, 2139, 2996,  319, 2708, 3280, 2196, 1903,  975,\n",
       "            2195, 2733, 1871, 1528, 1699, 3201,  959,  958,  850,   48, 1525,\n",
       "            2119, 1653, 1242,  677,  621,  722,  359, 1232, 1083,  330, 2056,\n",
       "            2130,   59,  905, 1077, 3222, 1828, 1876, 1385, 1387, 3202, 1633,\n",
       "            2762,  114, 1199, 3197,   60, 3436],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What are the indices of the 10 longest notes?\n",
    "# These are the longest character lengths, not necessarily the longest token lengths\n",
    "ten_longest_inputs_idx = headlines['sentence'].apply(lambda x: len(x)).sort_values(ascending=False).index[:50]\n",
    "\n",
    "ten_longest_inputs_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer\n",
      "The longest sequence is 133 tokens\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import heapq\n",
    "\n",
    "# Let's check how many tokens these are... Might not need all 512 tokens\n",
    "ten_longest_inputs = headlines['sentence'].iloc[ten_longest_inputs_idx].values\n",
    "\n",
    "print('Loading tokenizer')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"soleimanian/financial-roberta-large-sentiment\")\n",
    "\n",
    "token_lengths = []\n",
    "max_tokens = 0\n",
    "for input in ten_longest_inputs:\n",
    "    encodings = tokenizer(input,\n",
    "                            add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length=512,\n",
    "                            truncation=True,\n",
    "                            padding='do_not_pad')\n",
    "    input_ids = encodings.input_ids\n",
    "    max_tokens = max(max_tokens, len(input_ids))\n",
    "\n",
    "print(f'The longest sequence is {max_tokens} tokens')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Positive': 2, 'Neutral': 1, 'Negative': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {'Positive': 2,\n",
    "            'Neutral': 1,\n",
    "            'Negative': 0\n",
    "            }\n",
    "id2label = {0: 'Negative',\n",
    "            1:'Neutral',\n",
    "            2: 'Positive'}\n",
    "    \n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Negative', 1: 'Neutral', 2: 'Positive'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Train/Test Splits\n",
    "\n",
    "Use Stratified Shuffle Split to ensure equal class distribution among splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "X = headlines[['sentence']].values\n",
    "y= headlines.label.values\n",
    "\n",
    "# Define the split sizes\n",
    "split_sizes = [0.8, 0.1, 0.1]\n",
    "\n",
    "# Define the splitter with 2 splits\n",
    "splitter = StratifiedShuffleSplit(n_splits=2, test_size=split_sizes[-1], random_state=42)\n",
    "\n",
    "# Split the data into a train-dev set and a test set\n",
    "train_dev_indices, test_indices = next(splitter.split(X, y))\n",
    "\n",
    "# Get the train-dev data and the test data\n",
    "X_train_dev, y_train_dev = X[train_dev_indices], y[train_dev_indices]\n",
    "X_test, y_test = X[test_indices], y[test_indices]\n",
    "\n",
    "# Define the sizes of the train and dev sets\n",
    "train_size = int(split_sizes[0] * len(X_train_dev))\n",
    "dev_size = int(split_sizes[1] * len(X))\n",
    "\n",
    "# Split the train-dev set into a train set and a dev set\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=dev_size, random_state=42)\n",
    "train_indices, dev_indices = next(splitter.split(X_train_dev, y_train_dev))\n",
    "\n",
    "# Get the train data and the dev data\n",
    "X_train, y_train = X_train_dev[train_indices], y_train_dev[train_indices]\n",
    "X_dev, y_dev = X_train_dev[dev_indices], y_train_dev[dev_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the lists of sentences and their labels.\n",
    "train_sentences = [narr[0] for narr in X_train] \n",
    "train_labels = y_train\n",
    "dev_sentences = [narr[0] for narr in X_dev]\n",
    "dev_labels = y_dev\n",
    "test_sentences = [narr[0] for narr in X_test]\n",
    "test_labels = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization abstracts the news headlines into a structured form that RoBERTa can interpret. This involves breaking the text into smaller units, inserting [CLS] and [SEP] tokens to mark the beginning and end of sentences, padding shorter inputs with [PAD] tokens to a uniform length (133 length as discovered earlier), and truncating longer ones. Attention masks are also created to distinguish between actual inputs and padding. The breakdown and structuring of text in this manner enable the model to effectively understand unseen words, allocate attention appropriately, and capture the contextual relationships within the text, allowing it to make more accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def get_encodings(sentences):\n",
    "\n",
    "    sentence_dataset = Dataset.from_dict({'sentences':sentences } )\n",
    "    encodings = sentence_dataset.map(lambda x: tokenizer(\n",
    "                                                                x['sentences'], \n",
    "                                                                add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "                                                                max_length=max_tokens, # Replace this part with max_tokens for BigBird\n",
    "                                                                truncation=True,\n",
    "                                                                padding='max_length',\n",
    "                                                                ),\n",
    "                                            batched=True,\n",
    "                                            )\n",
    "\n",
    "    return encodings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = get_encodings(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = train_encodings[:]['input_ids']\n",
    "train_attention = train_encodings[:]['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_encodings = get_encodings(dev_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_inputs = dev_encodings[:]['input_ids']\n",
    "dev_attention = dev_encodings[:]['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encodings = get_encodings(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = test_encodings[:]['input_ids']\n",
    "test_attention = test_encodings[:]['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all inputs and labels into torch tensors, the required datatype \n",
    "# for our model\n",
    "\n",
    "import torch\n",
    "\n",
    "#Inputs\n",
    "train_inputs = torch.tensor(train_inputs, dtype=torch.int32)\n",
    "dev_inputs = torch.tensor(dev_inputs, dtype=torch.int32)\n",
    "test_inputs = torch.tensor(test_inputs, dtype=torch.int32)\n",
    "\n",
    "#Labels\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "dev_labels = torch.tensor(dev_labels, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "#Masks\n",
    "train_masks = torch.tensor(train_attention, dtype=torch.int32)\n",
    "dev_masks = torch.tensor(dev_attention, dtype=torch.int32)\n",
    "test_masks = torch.tensor(test_attention, dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset \n",
    "\n",
    "# Load the input ids, masks, etc. into separate datasets\n",
    "\n",
    "train_dataset = Dataset.from_dict({'input_ids':train_inputs,\n",
    "                                    'attention_mask': train_masks,\n",
    "                                    'labels': train_labels})\n",
    "\n",
    "dev_dataset =  Dataset.from_dict({'input_ids': dev_inputs,\n",
    "                                 'attention_mask': dev_masks,\n",
    "                                 'labels':dev_labels})\n",
    "\n",
    "test_dataset =  Dataset.from_dict({'input_ids':test_inputs,\n",
    "                                    'attention_mask':test_masks,\n",
    "                                    'labels':test_labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa (Robustly Optimized BERT Approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RoBERTa was chosen for this project to harness its advanced contextual understanding and exceptional performance in text classification tasks. RoBERTa is a high-performing transformer model developed by Facebook AI, excelling in numerous NLP tasks by leveraging attention mechanisms to understand the intricacies of language. Attention mechanisms enable the model to process and interpret the contextual relations and nuances between different parts of the input text. RoBERTa surpasses its predecessor, BERT, by using refined training techniques, larger datasets, and architectural improvements, which include the elimination of BERT's Next Sentence Prediction objective and dynamic adjustment of input token masking. This optimized training and enhanced contextual understanding make RoBERTa an excellent choice for this text classsification task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leveraging a Pre-Trained Model for Transfer Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Financial RoBERTa provides an opportunity for transfer learning, as it already acquired acquired an understanding of financial texts through fine-tuning on extensive financial documents like 10Ks and earnings call transcripts (important to note not the dataset used for this task). Using this specialized, pre-trained model facilitates more powerful predictions by effectively increasing the knowledge base of the text documents the model has to learn a language representation from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import AutoModel, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"soleimanian/financial-roberta-large-sentiment\", \n",
    "    num_labels = len(label2id),\n",
    "    problem_type = \"single_label_classification\",\n",
    "    hidden_dropout_prob=0.2,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    )\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_memory_footprint())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics: Accuracy, Macro/Weighted F1, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Function to calculate the accuracy and other metrics of our predictions vs labels\n",
    "def compute_metrics_multiclass(eval_pred):\n",
    "\n",
    "    # Labels give the ground truth for each *sentence* (num_classes, 1)\n",
    "    # Preds give the unnormalized probabilities for each class in a sentence (num_sentences, num_classes)\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten() # Get the index of the most probable classes (Already mapped index=class) and flatten to give one pred/sentence\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    print(\"Predicted Sample: \")\n",
    "    print(preds_flat[:10])\n",
    "    print(\"Actual Sample: \")\n",
    "    print(labels_flat[:10])\n",
    "\n",
    "    # Convert logits to float32 before applying softmax (flexibility for fp16 training)\n",
    "    preds_float32 = torch.tensor(preds, dtype=torch.float32)\n",
    "\n",
    "    # Convert logits to probabilities \n",
    "    preds_prob = torch.nn.functional.softmax(torch.tensor(preds_float32), dim=-1)\n",
    "\n",
    "    # Calculate the accuracy of the top 1 predictions\n",
    "    acc = accuracy_score(labels_flat, preds_flat)\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(labels_flat, preds_flat, average='weighted')\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(labels_flat, preds_flat, average='macro')\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision_weighted': precision_weighted,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_weighted': recall_weighted,\n",
    "        'recall_macro': recall_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'f1_macro': f1_macro\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In response to past experiences with extensive memory usage and slow computational speed during model training, adjustments were made to optimize resource efficiency. Mixed precision training (fp16) was employed, reducing memory requirements and accelerating computations. Concurrently, the 8BitAdam optimizer was used to compress model weights from 32 bits to 8 bits, conserving memory while preserving performance. Additionally, early stopping was implemented to prevent unnecessary computations and overfitting, contributing to a more efficient and dependable training process, essential for managing large datasets and advanced deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import multiprocessing as mp\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import bitsandbytes as bnb\n",
    "from bitsandbytes.optim import AdamW8bit\n",
    "from torch import nn\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Set threads==num cores on the machine\n",
    "torch.set_num_threads(mp.cpu_count())\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"Sentiment_Analysis_1stPass\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=15,        # Leave extra epochs in case learning brakes out of unoptimal convergence\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=False,\n",
    "    learning_rate=1e-6,\n",
    "    warmup_ratio = 0.4,  \n",
    "    fp16=True,  # gradients are computed in half precision & converted back to 32bit for the optimization step\n",
    "    disable_tqdm=False,\n",
    "    dataloader_pin_memory=True,\n",
    "    logging_dir='./logs',\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,  # Want smaller loss\n",
    "    load_best_model_at_end=True,\n",
    "    resume_from_checkpoint=False,\n",
    ")\n",
    "\n",
    "'''\n",
    "If you want to run a larger model and/or use a larger batch size, uncomment the bel\n",
    "'''\n",
    "decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
    "decay_parameters = [\n",
    "    name for name in decay_parameters if \"bias\" not in name]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n",
    "        \"weight_decay\": training_args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n",
    "        \"weight_decay\": 0.00,\n",
    "    },\n",
    "]\n",
    "\n",
    "optimizer_kwargs = {\n",
    "    \"betas\": (training_args.adam_beta1, training_args.adam_beta2),\n",
    "    \"eps\": training_args.adam_epsilon,\n",
    "}\n",
    "optimizer_kwargs[\"lr\"] = training_args.learning_rate\n",
    "adam_bnb_optim = bnb.optim.Adam8bit(\n",
    "    optimizer_grouped_parameters,\n",
    "    betas=(training_args.adam_beta1, training_args.adam_beta2),\n",
    "    eps=training_args.adam_epsilon,\n",
    "    lr=training_args.learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    optimizers=(adam_bnb_optim, None), #Uncomment accordinggly if you want 8 bit adam\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    compute_metrics=compute_metrics_multiclass,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5, early_stopping_threshold=0.01)],\n",
    "    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(r'Sentiment_Roberta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(r'Sentiment_Roberta')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
